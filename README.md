<p align="center">
  <b>âœ¦ Project II - COVID-19 Economic Analysis âœ¦</b><br>
  × ×™×ª×•×— ×”×©×¤×¢×ª ×”×§×•×¨×•× ×” ×¢×œ ××“×“×™ ×›×œ×›×œ×” ×¢×•×œ××™×™× (GDP, ××‘×˜×œ×” ×•-CPI)
</p>

---

## ğŸ“˜ Supervised Learning â€“ Regression Problem

×”×¤×¨×•×™×§×˜ ×× ×ª×— ××ª ×”×©×¤×¢×ª ××’×¤×ª ×”×§×•×¨×•× ×” ×¢×œ ×”×›×œ×›×œ×” ×”×¢×•×œ××™×ª ×‘×××¦×¢×•×ª × ×ª×•× ×™ COVID-19 ×•× ×ª×•× ×™× ×›×œ×›×œ×™×™× ×›××•:
**GDP**, **Unemployment**, ×•-**CPI**.

---

## ğŸ“Š ××˜×¨×ª ×”×¤×¨×•×™×§×˜
×”××˜×¨×” ×”×™× ×œ×—×–×•×ª ××ª **×”×ª×•×¦×¨ ×”××§×•××™ ×”×’×•×œ××™ (GDP)** ×©×œ ××“×™× ×•×ª ×©×•× ×•×ª,
×‘×”×ª×‘×¡×¡ ×¢×œ × ×ª×•× ×™ ×”×§×•×¨×•× ×” ×•×”××“×“×™× ×”×›×œ×›×œ×™×™× ×”× ×œ×•×•×™×.

---

## ğŸ§® ×§×•×“ ××œ× ×œ× ×™×ª×•×— ×•-Model Training

```python
# project_2
# Project by Israel Fadlon

# ============================================================
# ğŸ“˜ Supervised Learning - Regression Problem: COVID-19 & GDP
# ============================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV
from sklearn.model_selection import (
    train_test_split,
    cross_val_score,
    KFold
)
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_absolute_error, mean_squared_error
import joblib


# =======================
# 1ï¸âƒ£ DATA PREPARATION
# =======================

data = "/content/drive/MyDrive/Classroom/×¢×•×ª×§ ×©×œ Covid19_With_GDP_Values.csv"
df = pd.read_csv(data)
# ×›×œ ×”× ×ª×•× ×™× ×‘×¢××•×“×” ×”×–××ª ×©×”×™× ×”××—×•×– ×©×œ ×›×œ ××“×™× ×” ×”× "0" ××• ××™×œ×™× ×œ× ×¨×œ×•× ×˜×™×•×ª , ××– ×œ×“×˜× ×©×œ ×”××•×“×œ ××™×Ÿ ×¦×•×¨×š ×‘×”   
df = df.drop(columns=["Province/State", "Unnamed: 0"], errors='ignore')
df.dropna(subset=['CPI'], inplace=True)
df.drop_duplicates(inplace=True)

if 'Date' in df.columns:
    df['Year'] = pd.to_datetime(df['Date']).dt.year


# =======================
# 2ï¸âƒ£ DATA EXPLORATION
# =======================

corr = df.corr(numeric_only=True)
corr_with_gdp = corr['GDP'].sort_values(ascending=False)
print("ğŸ” Correlation with GDP:\n", corr_with_gdp)

important_cols = ['GDP', 'Confirmed', 'Deaths', 'Recovered', 'Unemployment', 'CPI']

df_2021 = df[df['Year'] == 2021][important_cols]
df_2022 = df[df['Year'] == 2022][important_cols]

plt.figure(figsize=(6,5))
sns.heatmap(df_2021.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap â€“ 2021")
plt.show()

plt.figure(figsize=(6,5))
sns.heatmap(df_2022.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap â€“ 2022")
plt.show()

```
# ğŸ“Š Correlation Analysis â€” ×ª×•×‘× ×•×ª ××¨×›×–×™×•×ª

## 1. ×§×©×¨ ×‘×™×Ÿ ××“×“×™ ×”×§×•×¨×•× ×” (COVID-19) ×œ-GDP

×‘×©× ×™× 2021â€“2022 × ××¦× ×›×™ ×”×ª×•×¦×¨ ×”×œ××•××™ (GDP) ××¦×™×’ **××ª×× ×—×œ×© ×××•×“** ×¢× ×›×œ ××“×“×™ ×”×§×•×¨×•× ×”:

- Confirmed â†” GDP â‰ˆ 0.24  
- Deaths â†” GDP â‰ˆ 0.22  
- Recovered â†” GDP â‰ˆ 0.13  

**××©××¢×•×ª:**  
×¨××ª ×”×ª×—×œ×•××” ××™× ×” ××¡×‘×™×¨×” ××ª ××¦×‘ ×”×›×œ×›×œ×” ×©×œ ×”××“×™× ×”.  
××“×™× ×•×ª ×‘×¢×œ×•×ª ×›×œ×›×œ×•×ª ×’×“×•×œ×•×ª ×•×§×˜× ×•×ª ×—×•×• ××ª ×”×§×•×¨×•× ×” ×‘×¦×•×¨×” ×©×•× ×” â€” ×•×œ×›×Ÿ ×”××ª×× ×‘×××•×¦×¢ × ××•×š ×××•×“.

×”-GDP ××•×©×¤×¢ ×‘×¢×™×§×¨ ××’×•×¨××™× ××‘× ×™×™× ×›×’×•×Ÿ:
- ×’×•×“×œ ×”×›×œ×›×œ×”  
- ××©××‘×™× ×˜×‘×¢×™×™×  
- ××“×™× ×™×•×ª ×××©×œ×ª×™×ª  
- ×¤×¢×™×œ×•×ª ××¡×—×¨ ×•×™×™×¦×•×¨  

×œ×›×Ÿ ××“×“×™ COVID ×œ×‘×“× ××™× × ××¦×œ×™×—×™× ×œ× ×‘× ××•×ª×•.

---

## 2. ×§×©×¨×™× ×‘×™×Ÿ ××“×“×™ ×”×§×•×¨×•× ×”

×›××Ÿ ××•×¤×™×¢×™× ×“×¤×•×¡×™× ×—×–×§×™× ×•×‘×¨×•×¨×™×:

- Confirmed â†” Recovered â‰ˆ 0.95  
- Confirmed â†” Deaths â‰ˆ 0.88â€“0.89  
- Deaths â†” Recovered â‰ˆ 0.79  

**××©××¢×•×ª:**  
××“×™× ×•×ª ×©×‘×”×Ÿ ××¡×¤×¨ ×”× ×“×‘×§×™× ×’×‘×•×” ×™×•×ª×¨ × ×•×˜×•×ª ×œ×”×¦×™×’ ×’× ××¡×¤×¨ ×’×‘×•×” ×©×œ ××—×œ×™××™× ×•××¡×¤×¨ ×’×‘×•×” ×©×œ × ×¤×˜×¨×™× â€” ×“×¤×•×¡ ×˜×‘×¢×™ ×‘××’×¤×•×ª.

---

## 3. ×§×©×¨ ×‘×™×Ÿ Unemployment ×•-CPI ×œ×©××¨ ×”××©×ª× ×™×

### Unemployment (××‘×˜×œ×”)
- ×§×©×¨ ×—×œ×© ×¢× GDP â‰ˆ âˆ’0.09  
- ×§×©×¨ ×›××¢×˜ ××¤×¡×™ ×¢× ××“×“×™ ×”×§×•×¨×•× ×”  

×”××‘×˜×œ×” ×‘×ª×§×•×¤×” ×–×• ×”×•×©×¤×¢×” ×™×•×ª×¨ ××”×—×œ×˜×•×ª ×××©×œ×ª×™×•×ª, ×¡×’×¨×™× ×•××“×™× ×™×•×ª ×ª×¢×¡×•×§×” â€” ×•×œ× ×™×©×™×¨×•×ª ××”×™×§×£ ×”×ª×—×œ×•××”.

### CPI (××“×“ ××—×™×¨×™×)
- ××ª×× ×›××¢×˜ ××¤×¡×™ ×¢× ×›×œ ×©××¨ ×”××©×ª× ×™× (â‰ˆ âˆ’0.04 ×¢×“ 0.04)  

×—×œ×§ ××”××“×™× ×•×ª ×—×¡×¨×•×ª ×¢×¨×›×™ CPI, ×•×‘×›×œ ××§×¨×” ×œ× × ××¦× ×“×¤×•×¡ ×›×œ×œ×™ ×©××¡×‘×™×¨ ××ª ×”×§×©×¨ ×‘×™×Ÿ ××“×“×™ ×”×‘×¨×™××•×ª ×œ××™× ×¤×œ×¦×™×” ×‘×ª×§×•×¤×” ×–×•.

---

## 4. ×”×©×•×•××” ×‘×™×Ÿ ×”×©× ×™× 2021 ×œ-2022

***×“×¤×•×¡×™ ×”×§×•×¨×œ×¦×™×” ×›××¢×˜ ×–×”×™× ×‘×©×ª×™ ×”×©× ×™×:***

- ×§×©×¨×™× ×—×–×§×™× ×‘×™×Ÿ Confirmedâ€“Deathsâ€“Recovered  
- ×§×©×¨ ×—×œ×© ×‘×™×Ÿ GDP ×œ××©×ª× ×™× ××—×¨×™×  
- CPI ×•××‘×˜×œ×” ×©×•××¨×™× ×¢×œ ××‘× ×” ×§×•×¨×œ×¦×™×” ×—×œ×© ×•×œ× ×¢×§×‘×™  

**××©××¢×•×ª:**  
× ×ª×•× ×™ ×”×‘×¨×™××•×ª ×©×œ ×”×§×•×¨×•× ×” ×œ× ×”×©×¤×™×¢×• ×‘×¦×•×¨×” ×™×©×™×¨×” ×¢×œ ×”×›×œ×›×œ×” ×©×œ ×”××“×™× ×•×ª ×’× ×‘×©× ×ª 2021 ×•×’× ×‘×©× ×ª 2022.

---

## ğŸ“Œ ××¡×§× ×” ×›×•×œ×œ×ª

- ××©×ª× ×™ ×”×§×•×¨×•× ×” ××ª×•×××™× ×××•×“ ×–×” ×œ×–×” â€” ×“×¤×•×¡ ××’×¤×” ×˜×‘×¢×™.  
- ××™×Ÿ ×§×©×¨ ××•×‘×”×§ ×‘×™×Ÿ COVID-19 ×œ-GDP.  
- CPI ×•××‘×˜×œ×” ×›××¢×˜ ×©××™× × ××¦×™×’×™× ×§×©×¨×™× ××•×‘×”×§×™× ×œ×©××¨ ×”××©×ª× ×™×.  

×œ×›×Ÿ, ×”×”×©×¤×¢×” ×”×›×œ×›×œ×™×ª ×©×œ ×”×§×•×¨×•× ×” **××™× ×” ××•×¤×™×¢×” ×‘×¦×•×¨×” ××•×‘×”×§×ª** ×‘×××’×¨ ×”× ×ª×•× ×™× ×”×–×”, ×•×–×” ××¡×‘×™×¨ ××“×•×¢ ××•×“×œ×™ ×”×¨×’×¨×¡×™×” ××¦×™×’×™× RÂ² × ××•×š â€” ×ª×•×¦××” ×ª×§×™× ×” ×œ×—×œ×•×˜×™×Ÿ ×‘×”×ª×—×©×‘ ×‘× ×ª×•× ×™×.







# LINEAR REGRESSION MULTIE MODEL TRAINING

```
# ×˜×¢×™× ×” ×•×”×›× ×” ×‘×¡×™×¡×™×ª ×©×œ ×”× ×ª×•× ×™×
if 'Year' not in df.columns and 'Date' in df.columns:
    df['Year'] = pd.to_datetime(df['Date']).dt.year
df = df[df['Year'].isin([2021, 2022])].copy() 

# ×”×’×“×¨×ª ×”×¢××•×“×•×ª ×”×—×©×•×‘×•×ª ×œ××•×“×œ
model_numeric_cols = ['Confirmed', 'Deaths', 'Recovered', 'Unemployment', 'CPI', 'GDP']

# ×”×’×“×¨×ª ×”×¢××•×“×•×ª ×œ××¡×¤×¨×™×
for col in model_numeric_cols:
    df[col] = pd.to_numeric(df[col], errors='coerce')

# ×˜×™×¤×•×œ ×‘×¢×¨×›×™× ×—×¡×¨×™×
df[model_numeric_cols] = df[model_numeric_cols].fillna(df[model_numeric_cols].mean())

# ×‘× ×™×™×¦ ××•×“×œ ×œ×©× ×™× 2021 ,2022 
for year in [2021, 2022]:
    print(f"\nğŸ§© ===== × ×™×ª×•×— ×¢×‘×•×¨ ×”×©× ×” {year} ====")

    df_year = df[df['Year'] == year]

    #×”×’×“×¨×ª ××©×ª× ×™× ×œ××•×“×œ 
    X = df_year[['Confirmed', 'Deaths', 'Recovered', 'Unemployment', 'CPI']]
    y = df_year['GDP']

    #  × ×¨××•×œ ×œ×¡×˜×™×™×ª ×ª×§×Ÿ 1 ×•×××•×¦×¢ 0
    scaler = StandardScaler()
    X_scaled = scaler.fit_transform(X)

    #  ×—×™×œ×•×§ ×©×œ ×”× ×ª×•× ×™× ×œ 70%  ××™××•×Ÿ ×”××•×“×œ ,×• 30% ×‘×“×™×§×” ×××™×ª×™×ª
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.3, random_state=42
    )

    # 2 # 

    # ××—×©×‘ ××ª ×”×§×©×¨ ×‘×™×Ÿ ×”××©×ª× ×™× (X) ×œ×‘×™×Ÿ ×”×ª×•×¦×¨ (GDP).
    model = LinearRegression()
    model.fit(X_train, y_train)

    # 3 #

    # ×ª×•×¦××•×ª
    train_score = model.score(X_train, y_train)
    test_score = model.score(X_test, y_test)
    cv_scores = cross_val_score(model, X_scaled, y, cv=5, scoring='r2')

    print(f"ğŸ“Š Train RÂ²: {train_score:.3f}")
    print(f"ğŸ“ˆ Test RÂ²: {test_score:.3f}")
    print("ğŸ” Cross Validation RÂ² scores:", cv_scores)
    print("â­ Average RÂ²:", np.mean(cv_scores))


#ğŸ§© ===== × ×™×ª×•×— ×¢×‘×•×¨ ×”×©× ×” 2021 ====
#ğŸ“Š Train RÂ²: 0.052
#ğŸ“ˆ Test RÂ²: 0.360
#ğŸ” Cross Validation RÂ² scores: [-3.62191755e-02  4.37463648e-01  3.09127470e-01 -6.32888672e-01
# -8.91820078e+01]
#â­ Average RÂ²: -17.820904911486192

#ğŸ§© ===== × ×™×ª×•×— ×¢×‘×•×¨ ×”×©× ×” 2022 ====
#ğŸ“Š Train RÂ²: 0.054
#ğŸ“ˆ Test RÂ²: 0.257
#ğŸ” Cross Validation RÂ² scores: [-3.80816578e-02  3.98185317e-01 -2.47026004e-01 -1.23598581e-01
# -5.64311892e+01]
#â­ Average RÂ²: -11.288342021292618


# Multi model training 

# 1 # 


# ×××¤×™×™× ×™× ×•×ª×™×•×’
X = df[['Confirmed', 'Deaths', 'Recovered', 'Unemployment', 'CPI']]
y = df['GDP']

# × ×¨××•×œ
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ××•×“×œ ×œ×™× ×™××¨×™ ×¨×’×™×œ
lin_model = LinearRegression()

# K-Fold Cross Validation
kfold = KFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(lin_model, X_scaled, y, cv=kfold, scoring='r2')

print("Linear Regression CV RÂ² Scores:", scores)
print("Average RÂ²:", np.mean(scores))

# 2 # 


# ×œ×”×•×¡×™×£ ×¨×’×•×œ×¨×™×–×¦×™×” (L2 penalty) ×©××•× ×¢×ª ××”××•×“×œ â€œ×œ×”×’×–×™×â€ ×¢× ××§×“××™× ×’×“×•×œ×™× ××“×™.
# ×–×” ×¢×•×–×¨ ×‘××§×¨×™× ×©×œ multicollinearity ××• × ×ª×•× ×™× ×¨×•×¢×©×™×.
alphas = np.logspace(-3, 3, 50)


ridge_model = RidgeCV(alphas=alphas, cv=5, scoring='r2')
ridge_model.fit(X_scaled, y)

# ×‘×“×™×§×”
print(f"Optimal alpha (Î»): {ridge_model.alpha_}")
print(f"RÂ² Score (using best Î»): {ridge_model.score(X_scaled, y):.3f}")

# 3 # 



# ×˜×•×•×— ×¢×¨×›×™× ×©×œ Î» (××œ×¤×)
alphas = np.logspace(-3, 3, 50)

lasso_model = LassoCV(alphas=alphas, cv=5, random_state=42)
lasso_model.fit(X_scaled, y)

print(f"Optimal alpha (Î»): {lasso_model.alpha_}")
print(f"RÂ² Score (using best Î»): {lasso_model.score(X_scaled, y):.3f}")


# ×¤×” ××¤×©×¨ ×’× ×œ×¨××•×ª ×›××” ××§×“××™× × ×©××¨×• ×¢× !=0
print("Number of features kept:", np.sum(lasso_model.coef_ != 0))

# 4 # 

# ×œ××¤×©×¨ ×œ××•×“×œ â€œ×œ×”×ª×›×•×¤×£â€ â€” ×›×œ×•××¨ ×œ×–×”×•×ª ×§×©×¨×™× ×œ× ×œ×™× ×™××¨×™×™× ×‘×™×Ÿ ×”××©×ª× ×™×


degrees = [1, 2, 3, 4, 5]
avg_scores = []

for d in degrees:
    poly_model = make_pipeline(PolynomialFeatures(d), LinearRegression())
    score = cross_val_score(poly_model, X_scaled, y, cv=5, scoring='r2').mean()
    avg_scores.append(score)
    print(f"Degree {d} â†’ Mean RÂ²: {score:.3f}")

# ×‘×—×™×¨×ª ×”×“×¨×’×” ×”×˜×•×‘×” ×‘×™×•×ª×¨
best_degree = degrees[np.argmax(avg_scores)]
print(f"\nOptimal Polynomial Degree: {best_degree}")





# ××•×ª× ×××¤×™×™× ×™× ×•×ª×™×•×’×™×
X = df[['Confirmed', 'Deaths', 'Recovered', 'Unemployment', 'CPI']]
y = df['GDP']

# × ×¨××•×œ
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# ×”×’×“×¨×•×ª ××•×¤×˜×™××œ×™×•×ª ××”×©×œ×‘ ×”×§×•×“×
ridge_opt_alpha =  ridge_model.alpha_
lasso_opt_alpha = lasso_model.alpha_
best_degree = best_degree  # ××”×¤×•×œ×™× ×•××™ ×”×§×•×“×

# ×‘× ×™×™×ª ×”××•×“×œ×™× ×”×¡×•×¤×™×™×
models = {
    "Linear Regression": LinearRegression(),
    "RidgeCV": RidgeCV(alphas=[ridge_opt_alpha]),
    "LassoCV": LassoCV(alphas=[lasso_opt_alpha]),
    f"Polynomial (deg={best_degree})": make_pipeline(PolynomialFeatures(best_degree), LinearRegression())
}



print("===== Optimal Parameters & Coefficients =====")
for name, model in models.items():
    model.fit(X_scaled, y)
    print(f"\nğŸ”¹ {name}")
    if hasattr(model, "alpha_"):
        print(f"Optimal Î» (alpha): {model.alpha_}")
    if hasattr(model, "coef_"):
        print("Beta Coefficients:")
        for feature, coef in zip(['Confirmed', 'Deaths', 'Recovered', 'Unemployment', 'CPI'], model.coef_):
            print(f"  {feature}: {coef:.4f}")
    elif hasattr(model[-1], "coef_"):  # ×œ××•×“×œ×™× ×¢× pipeline
        print("Beta Coefficients (Polynomial):")
        print(model[-1].coef_)

results = []

for name, model in models.items():
    y_pred = model.predict(X_scaled)
    mae = mean_absolute_error(y, y_pred)
    mse = mean_squared_error(y, y_pred)
    rmse = np.sqrt(mse)
    r2 = model.score(X_scaled, y)
    results.append([name, mae, mse, rmse, r2])

results_df = pd.DataFrame(results, columns=['Model', 'MAE', 'MSE', 'RMSE', 'RÂ²'])
print("\n===== Model Evaluation =====")
print(results_df)

plt.figure(figsize=(8,5))
plt.bar(results_df['Model'], results_df['RÂ²'], color='skyblue')
plt.title('ğŸ“ˆ Model Accuracy (RÂ² Comparison)')
plt.ylabel('RÂ² Score')
plt.xticks(rotation=30)
plt.show()


best_model_name = results_df.loc[results_df['RÂ²'].idxmax(), 'Model']
print(f"\nâœ… Best Performing Model: {best_model_name}")


best_model = models[best_model_name]
best_model.fit(X_scaled, y)
print("\nğŸ Final model trained on full dataset!")

    


joblib.dump(best_model, "final_model.joblib")
joblib.dump(scaler, "scaler.joblib")

# ×‘××§×¨×” ×©×œ ××•×“×œ ×¤×•×œ×™× ×•××™ â€“ ×©××•×¨ ×’× ××ª ×”-Polynomial Converter
if "Polynomial" in best_model_name:
    joblib.dump(best_model.named_steps['polynomialfeatures'], "poly_converter.joblib")

print("\nğŸ’¾ Model and preprocessing saved successfully!")


loaded_model = joblib.load("final_model.joblib")
loaded_scaler = joblib.load("scaler.joblib")

if "Polynomial" in best_model_name:
    loaded_poly = joblib.load("poly_converter.joblib")
    print("âœ… Polynomial converter loaded too!")

print("\nğŸš€ Model and preprocessing reloaded successfully and ready for inference.")


```

## ğŸ“˜ Summary & Discussion

×œ××—×¨ ×‘×“×™×§×ª ×›×œ×œ ×”××•×“×œ×™× (Linear, Ridge, Lasso, Polynomial), ××ª×§×‘×œ ×›×™ ×¢×¨×›×™ ×”Ö¾RÂ² × ××•×›×™× ×™×—×¡×™×ª.
×”××©××¢×•×ª ×”×™× ×©××“×“×™ ×”×‘×¨×™××•×ª ×©×œ ×”×§×•×¨×•× ×” (Confirmed, Deaths, Recovered) ×•×›×Ÿ ××“×“×™ ×”×××§×¨×• ×”×‘×¡×™×¡×™×™×
(CPI, Unemployment) **××™× × ××¡×•×’×œ×™× ×œ×”×¡×‘×™×¨ ×‘×¦×•×¨×” ×˜×•×‘×” ××ª ×”×©×•× ×•×ª ×‘-GDP ×‘×™×Ÿ ×”××“×™× ×•×ª**.

×›×œ ××¨×‘×¢×ª ×”××•×“×œ×™× ××¦×™×’×™× ×‘×™×¦×•×¢×™× ×“×•××™× ×××•×“, ×›××©×¨:
- **Linear Regression** ××¦×™×’ ××ª ×¢×¨×š ×”Ö¾RÂ² ×”×’×‘×•×” ×‘×™×•×ª×¨ (×’× ×× ×‘×¤×¢×¨ ×§×˜×Ÿ ×××•×“).  
- ×”×•×¡×¤×ª ×¨×’×•×œ×¨×™×–×¦×™×” (Ridge/Lasso) ×œ× ×©×™×¤×¨×” ××ª ×”×‘×™×¦×•×¢×™× ×‘××•×¤×Ÿ ××©××¢×•×ª×™.
- Polynomial Regression ×‘×“×¨×’×•×ª ×’×‘×•×”×•×ª ×§×¨×¡ ×œ×—×œ×•×˜×™×Ÿ (overfitting) â€” ×•×¨×§ ×“×¨×’×” 1 ×¢×‘×“×”, ×œ××¢×©×” ×›××• ××•×“×œ ×œ×™× ×™××¨×™.

### ğŸ¯ ××¡×§× ×”:
×”× ×ª×•× ×™× ××¨××™× ×›×™ **××™×Ÿ ×§×©×¨ ××•×‘×”×§ ×‘×™×Ÿ ××“×“×™ COVID-19 ×œ×‘×™×Ÿ ×¨××ª ×”×ª×•×¦×¨ (GDP)**.
×œ×›×Ÿ ××•×“×œ×™× ×œ×™× ×™××¨×™×™× ××• ×¤×•×œ×™× ×•××™×™× ××™× × ××¡×•×’×œ×™× ×œ×—×–×•×ª ××ª ×”-GDP ×‘×¦×•×¨×” ×˜×•×‘×” ×‘×¢×–×¨×ª ××©×ª× ×™× ××œ×• ×‘×œ×‘×“.

×›×“×™ ×œ×©×¤×¨ ××ª ×”×‘×™×¦×•×¢×™× ×™×© ×¦×•×¨×š ×œ×”×•×¡×™×£ ××©×ª× ×™× ×—×™×¦×•× ×™×™× ×›×’×•×Ÿ:
- ×’×•×“×œ ××•×›×œ×•×¡×™×™×”  
- ×”×™×§×£ ××¡×—×¨ ×‘×™× ×œ××•××™  
- ×”×•×¦××” ×××©×œ×ª×™×ª  
- ×©×™×¢×•×¨×™ ×¦××™×—×” ×§×•×“××™×  
- ×—×•×‘ ×œ××•××™  
- ××“×“×™ ×¤×™×ª×•×— (HDI)  

××•×“×œ×™× ××‘×•×¡×¡×™ ××’××•×ª ×××§×¨×•Ö¾×›×œ×›×œ×™×•×ª ×™×ª××™××• ×”×¨×‘×” ×™×•×ª×¨ ×œ××©×™××”.





```python
# ğŸ“Œ Customer Churn Prediction â€“ Full Project Code
## âœ”ï¸ ×›×•×œ×œ Data Preparation, EDA, Training, Evaluation, Deployment

```python
# =========================================================
# ğŸ“¦ Imports â€“ ×›×œ ×”×™×™×‘×•× ××¨×•×›×– ×‘×ª×—×™×œ×ª ×”×§×•×“
# =========================================================
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, recall_score, f1_score,
    confusion_matrix
)
import joblib

# =========================================================
# ğŸ“Œ Data Preparation
# =========================================================

# ×˜×¢×™× ×ª ×”× ×ª×•× ×™×
path = "/content/drive/MyDrive/Classroom/×¢×•×ª×§ ×©×œ customer_churn_dataset.csv"
df = pd.read_csv(path)

df.head()
df.columns

# ×¢××•×“×ª CustomerID ×”×™× ××–×”×” ×‘×œ×‘×“ â†’ ×œ× × ×•×ª× ×ª ××™×“×¢ ×¢×œ ×”×ª× ×”×’×•×ª â†’ ××¡×™×¨×™×
df = df.drop(columns=["CustomerID"])
df.columns

# ×‘×“×™×§×ª ×¢×¨×›×™× ×—×¡×¨×™×
print("Missing Values:")
print(df.isnull().sum())

# ×‘×“×™×§×ª ×›×¤×™×œ×•×™×•×ª
print("Duplicate Rows:", df.duplicated().sum())

# ×§×™×“×•×“ ××©×ª× ×™× ×§×˜×’×•×¨×™××œ×™×™×
df = pd.get_dummies(df, drop_first=True)
df.head()


# =========================================================
# ğŸ“Š Data Exploration
# =========================================================

# ×—×™×©×•×‘ ×§×•×¨×œ×¦×™×”
corr = df.corr()
corr

# Heatmap
plt.figure(figsize=(14, 10))
sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()

# Pairplot
sns.pairplot(df, height=2)
plt.show()


# =========================================================
# ğŸ“Œ Model Training â€“ Scaling + Splitting
# =========================================================

# X & y
X = df.drop("Churn", axis=1)
y = df["Churn"]

# ×—×œ×•×§×” ×œÖ¾Train/Test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# × ×¨××•×œ (Standardization)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_scaled[:5]


# =========================================================
# ğŸ¤– Logistic Regression â€“ Grid Search
# =========================================================

log_params = {
    'C': [0.01, 0.1, 1, 10],
    'penalty': ['l2'],
    'solver': ['lbfgs']
}

log_model = LogisticRegression(max_iter=500)
log_grid = GridSearchCV(log_model, log_params, cv=5, scoring='f1')
log_grid.fit(X_train_scaled, y_train)

print("Optimal Logistic Regression Parameters:")
print(log_grid.best_params_)

log_best = log_grid.best_estimator_


# =========================================================
# ğŸ¤– KNN â€“ Grid Search
# =========================================================

knn_params = {
    'n_neighbors': list(range(1, 21)),
    'weights': ['uniform', 'distance']
}

knn = KNeighborsClassifier()
knn_grid = GridSearchCV(knn, knn_params, cv=5, scoring='f1')
knn_grid.fit(X_train_scaled, y_train)

print("Optimal KNN Parameters:")
print(knn_grid.best_params_)

knn_best = knn_grid.best_estimator_


# =========================================================
# ğŸ¤– SVM â€“ Grid Search
# =========================================================

svm_params = {
    'C': [0.1, 1, 10],
    'kernel': ['rbf', 'linear'],
    'gamma': ['scale', 'auto']
}

svm = SVC(probability=True)
svm_grid = GridSearchCV(svm, svm_params, cv=5, scoring='f1')
svm_grid.fit(X_train_scaled, y_train)

print("Optimal SVM Parameters:")
print(svm_grid.best_params_)

svm_best = svm_grid.best_estimator_


# =========================================================
# ğŸŒ² Random Forest â€“ Grid Search
# =========================================================

rf_params = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 10, None],
    'min_samples_split': [2, 5, 10],
    'bootstrap': [True, False]
}

rf = RandomForestClassifier()
rf_grid = GridSearchCV(rf, rf_params, cv=5, scoring='f1')
rf_grid.fit(X_train, y_train)

print("Optimal Random Forest Parameters:")
print(rf_grid.best_params_)

rf_best = rf_grid.best_estimator_


# =========================================================
# ğŸ“Œ Predictions (×œ×¤×™ ×“×¨×™×©×•×ª ×”×¤×¨×•×™×§×˜)
# =========================================================

y_pred_log = log_best.predict(X_test_scaled)
y_proba_log = log_best.predict_proba(X_test_scaled)[:, 1]

y_pred_knn = knn_best.predict(X_test_scaled)
y_proba_knn = knn_best.predict_proba(X_test_scaled)[:, 1]

y_pred_svm = svm_best.predict(X_test_scaled)
y_proba_svm = svm_best.predict_proba(X_test_scaled)[:, 1]

y_pred_rf = rf_best.predict(X_test)
y_proba_rf = rf_best.predict_proba(X_test)[:, 1]


# =========================================================
# ğŸ“Œ Model Evaluation â€“ Accuracy, Recall, F1
# =========================================================

results = {"Model": [], "Accuracy": [], "Recall": [], "F1 Score": []}

def evaluate_model(name, y_test, y_pred):
    acc = accuracy_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    results["Model"].append(name)
    results["Accuracy"].append(acc)
    results["Recall"].append(rec)
    results["F1 Score"].append(f1)

    print(f"\n{name}:")
    print("Accuracy:", acc)
    print("Recall:", rec)
    print("F1 Score:", f1)


evaluate_model("Logistic Regression", y_test, y_pred_log)
evaluate_model("KNN", y_test, y_pred_knn)
evaluate_model("SVM", y_test, y_pred_svm)
evaluate_model("Random Forest", y_test, y_pred_rf)


# =========================================================
# ğŸ“Œ Confusion Matrices
# =========================================================

def plot_confusion(model_name, y_test, y_pred):
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, cmap="Blues", fmt="d")
    plt.title(f"Confusion Matrix â€“ {model_name}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

plot_confusion("Logistic Regression", y_test, y_pred_log)
plot_confusion("KNN", y_test, y_pred_knn)
plot_confusion("SVM", y_test, y_pred_svm)
plot_confusion("Random Forest", y_test, y_pred_rf)


# =========================================================
# ğŸ“Œ Model Comparison Table
# =========================================================

results_df = pd.DataFrame(results)
results_df


# =========================================================
# ğŸ† Selecting the Best Model
# =========================================================

best_model_name = results_df.iloc[results_df["F1 Score"].idxmax()]["Model"]
print("Best Model:", best_model_name)


# =========================================================
# ğŸ¯ Training Best Model on Full Dataset
# =========================================================

if best_model_name == "Logistic Regression":
    final_model = log_best
    X_all_scaled = scaler.fit_transform(X)
    final_model.fit(X_all_scaled, y)

elif best_model_name == "KNN":
    final_model = knn_best
    X_all_scaled = scaler.fit_transform(X)
    final_model.fit(X_all_scaled, y)

elif best_model_name == "SVM":
    final_model = svm_best
    X_all_scaled = scaler.fit_transform(X)
    final_model.fit(X_all_scaled, y)

elif best_model_name == "Random Forest":
    final_model = rf_best
    final_model.fit(X, y)  # RF ×œ× ×—×™×™×‘ Scaling


# =========================================================
# ğŸ’¾ Export Model + Scaler
# =========================================================

joblib.dump(final_model, "final_model.joblib")
joblib.dump(scaler, "scaler.joblib")

print("Model and scaler saved successfully!")


# =========================================================
# ğŸ”„ Loading Saved Model Back
# =========================================================

loaded_model = joblib.load("final_model.joblib")
loaded_scaler = joblib.load("scaler.joblib")

print("Model and scaler loaded successfully!")
```
