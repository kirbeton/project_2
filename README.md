<p align="center">
  <b>âœ¦ Project II - COVID-19 Economic Analysis âœ¦</b><br>
  × ×™×ª×•×— ×”×©×¤×¢×ª ×”×§×•×¨×•× ×” ×¢×œ ××“×“×™ ×›×œ×›×œ×” ×¢×•×œ××™×™× (GDP, ××‘×˜×œ×” ×•-CPI)
</p>

---

## ğŸ“˜ Supervised Learning â€“ Regression Problem

×”×¤×¨×•×™×§×˜ ×× ×ª×— ××ª ×”×©×¤×¢×ª ××’×¤×ª ×”×§×•×¨×•× ×” ×¢×œ ×”×›×œ×›×œ×” ×”×¢×•×œ××™×ª ×‘×××¦×¢×•×ª × ×ª×•× ×™ COVID-19 ×•× ×ª×•× ×™× ×›×œ×›×œ×™×™× ×›××•:
**GDP**, **Unemployment**, ×•-**CPI**.

---

## ğŸ“Š ××˜×¨×ª ×”×¤×¨×•×™×§×˜
×”××˜×¨×” ×”×™× ×œ×—×–×•×ª ××ª **×”×ª×•×¦×¨ ×”××§×•××™ ×”×’×•×œ××™ (GDP)** ×©×œ ××“×™× ×•×ª ×©×•× ×•×ª,
×‘×”×ª×‘×¡×¡ ×¢×œ × ×ª×•× ×™ ×”×§×•×¨×•× ×” ×•×”××“×“×™× ×”×›×œ×›×œ×™×™× ×”× ×œ×•×•×™×.

---

## ğŸ§® ×§×•×“ ××œ× ×œ× ×™×ª×•×— ×•-Model Training

```python
# project_2
# Project by Israel Fadlon

# ============================================================
# ğŸ“˜ Supervised Learning - Regression Problem: COVID-19 & GDP
# ============================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.linear_model import LinearRegression, LassoCV, RidgeCV
from sklearn.model_selection import (
    train_test_split,
    cross_val_score,
    KFold
)
from sklearn.preprocessing import StandardScaler, PolynomialFeatures
from sklearn.pipeline import make_pipeline
from sklearn.metrics import mean_absolute_error, mean_squared_error
import joblib


# =======================
# 1ï¸âƒ£ DATA PREPARATION
# =======================

data = "/content/drive/MyDrive/Classroom/×¢×•×ª×§ ×©×œ Covid19_With_GDP_Values.csv"
df = pd.read_csv(data)
# ×›×œ ×”× ×ª×•× ×™× ×‘×¢××•×“×” ×”×–××ª ×©×”×™× ×”××—×•×– ×©×œ ×›×œ ××“×™× ×” ×”× "0" ××• ××™×œ×™× ×œ× ×¨×œ×•× ×˜×™×•×ª , ××– ×œ×“×˜× ×©×œ ×”××•×“×œ ××™×Ÿ ×¦×•×¨×š ×‘×”   
df = df.drop(columns=["Province/State", "Unnamed: 0"], errors='ignore')
df.dropna(subset=['CPI'], inplace=True)
df.drop_duplicates(inplace=True)

if 'Date' in df.columns:
    df['Year'] = pd.to_datetime(df['Date']).dt.year


# =======================
# 2ï¸âƒ£ DATA EXPLORATION
# =======================

corr = df.corr(numeric_only=True)
corr_with_gdp = corr['GDP'].sort_values(ascending=False)
print("ğŸ” Correlation with GDP:\n", corr_with_gdp)

important_cols = ['GDP', 'Confirmed', 'Deaths', 'Recovered', 'Unemployment', 'CPI']

df_2021 = df[df['Year'] == 2021][important_cols]
df_2022 = df[df['Year'] == 2022][important_cols]

plt.figure(figsize=(6,5))
sns.heatmap(df_2021.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap â€“ 2021")
plt.show()

plt.figure(figsize=(6,5))
sns.heatmap(df_2022.corr(numeric_only=True), annot=True, cmap='coolwarm', fmt=".2f")
plt.title("Correlation Heatmap â€“ 2022")
plt.show()

```
# ğŸ“Š Correlation Analysis â€” ×ª×•×‘× ×•×ª ××¨×›×–×™×•×ª

## 1. ×§×©×¨ ×‘×™×Ÿ ××“×“×™ ×”×§×•×¨×•× ×” (COVID-19) ×œ-GDP

×‘×©× ×™× 2021â€“2022 × ××¦× ×›×™ ×”×ª×•×¦×¨ ×”×œ××•××™ (GDP) ××¦×™×’ **××ª×× ×—×œ×© ×××•×“** ×¢× ×›×œ ××“×“×™ ×”×§×•×¨×•× ×”:

- Confirmed â†” GDP â‰ˆ 0.24  
- Deaths â†” GDP â‰ˆ 0.22  
- Recovered â†” GDP â‰ˆ 0.13  

**××©××¢×•×ª:**  
×¨××ª ×”×ª×—×œ×•××” ××™× ×” ××¡×‘×™×¨×” ××ª ××¦×‘ ×”×›×œ×›×œ×” ×©×œ ×”××“×™× ×”.  
××“×™× ×•×ª ×‘×¢×œ×•×ª ×›×œ×›×œ×•×ª ×’×“×•×œ×•×ª ×•×§×˜× ×•×ª ×—×•×• ××ª ×”×§×•×¨×•× ×” ×‘×¦×•×¨×” ×©×•× ×” â€” ×•×œ×›×Ÿ ×”××ª×× ×‘×××•×¦×¢ × ××•×š ×××•×“.

×”-GDP ××•×©×¤×¢ ×‘×¢×™×§×¨ ××’×•×¨××™× ××‘× ×™×™× ×›×’×•×Ÿ:
- ×’×•×“×œ ×”×›×œ×›×œ×”  
- ××©××‘×™× ×˜×‘×¢×™×™×  
- ××“×™× ×™×•×ª ×××©×œ×ª×™×ª  
- ×¤×¢×™×œ×•×ª ××¡×—×¨ ×•×™×™×¦×•×¨  

×œ×›×Ÿ ××“×“×™ COVID ×œ×‘×“× ××™× × ××¦×œ×™×—×™× ×œ× ×‘× ××•×ª×•.

---

## 2. ×§×©×¨×™× ×‘×™×Ÿ ××“×“×™ ×”×§×•×¨×•× ×”

×›××Ÿ ××•×¤×™×¢×™× ×“×¤×•×¡×™× ×—×–×§×™× ×•×‘×¨×•×¨×™×:

- Confirmed â†” Recovered â‰ˆ 0.95  
- Confirmed â†” Deaths â‰ˆ 0.88â€“0.89  
- Deaths â†” Recovered â‰ˆ 0.79  

**××©××¢×•×ª:**  
××“×™× ×•×ª ×©×‘×”×Ÿ ××¡×¤×¨ ×”× ×“×‘×§×™× ×’×‘×•×” ×™×•×ª×¨ × ×•×˜×•×ª ×œ×”×¦×™×’ ×’× ××¡×¤×¨ ×’×‘×•×” ×©×œ ××—×œ×™××™× ×•××¡×¤×¨ ×’×‘×•×” ×©×œ × ×¤×˜×¨×™× â€” ×“×¤×•×¡ ×˜×‘×¢×™ ×‘××’×¤×•×ª.

---

## 3. ×§×©×¨ ×‘×™×Ÿ Unemployment ×•-CPI ×œ×©××¨ ×”××©×ª× ×™×

### Unemployment (××‘×˜×œ×”)
- ×§×©×¨ ×—×œ×© ×¢× GDP â‰ˆ âˆ’0.09  
- ×§×©×¨ ×›××¢×˜ ××¤×¡×™ ×¢× ××“×“×™ ×”×§×•×¨×•× ×”  

×”××‘×˜×œ×” ×‘×ª×§×•×¤×” ×–×• ×”×•×©×¤×¢×” ×™×•×ª×¨ ××”×—×œ×˜×•×ª ×××©×œ×ª×™×•×ª, ×¡×’×¨×™× ×•××“×™× ×™×•×ª ×ª×¢×¡×•×§×” â€” ×•×œ× ×™×©×™×¨×•×ª ××”×™×§×£ ×”×ª×—×œ×•××”.

### CPI (××“×“ ××—×™×¨×™×)
- ××ª×× ×›××¢×˜ ××¤×¡×™ ×¢× ×›×œ ×©××¨ ×”××©×ª× ×™× (â‰ˆ âˆ’0.04 ×¢×“ 0.04)  

×—×œ×§ ××”××“×™× ×•×ª ×—×¡×¨×•×ª ×¢×¨×›×™ CPI, ×•×‘×›×œ ××§×¨×” ×œ× × ××¦× ×“×¤×•×¡ ×›×œ×œ×™ ×©××¡×‘×™×¨ ××ª ×”×§×©×¨ ×‘×™×Ÿ ××“×“×™ ×”×‘×¨×™××•×ª ×œ××™× ×¤×œ×¦×™×” ×‘×ª×§×•×¤×” ×–×•.

---

## 4. ×”×©×•×•××” ×‘×™×Ÿ ×”×©× ×™× 2021 ×œ-2022

***×“×¤×•×¡×™ ×”×§×•×¨×œ×¦×™×” ×›××¢×˜ ×–×”×™× ×‘×©×ª×™ ×”×©× ×™×:***

- ×§×©×¨×™× ×—×–×§×™× ×‘×™×Ÿ Confirmedâ€“Deathsâ€“Recovered  
- ×§×©×¨ ×—×œ×© ×‘×™×Ÿ GDP ×œ××©×ª× ×™× ××—×¨×™×  
- CPI ×•××‘×˜×œ×” ×©×•××¨×™× ×¢×œ ××‘× ×” ×§×•×¨×œ×¦×™×” ×—×œ×© ×•×œ× ×¢×§×‘×™  

**××©××¢×•×ª:**  
× ×ª×•× ×™ ×”×‘×¨×™××•×ª ×©×œ ×”×§×•×¨×•× ×” ×œ× ×”×©×¤×™×¢×• ×‘×¦×•×¨×” ×™×©×™×¨×” ×¢×œ ×”×›×œ×›×œ×” ×©×œ ×”××“×™× ×•×ª ×’× ×‘×©× ×ª 2021 ×•×’× ×‘×©× ×ª 2022.

---

## ğŸ“Œ ××¡×§× ×” ×›×•×œ×œ×ª

- ××©×ª× ×™ ×”×§×•×¨×•× ×” ××ª×•×××™× ×××•×“ ×–×” ×œ×–×” â€” ×“×¤×•×¡ ××’×¤×” ×˜×‘×¢×™.  
- ××™×Ÿ ×§×©×¨ ××•×‘×”×§ ×‘×™×Ÿ COVID-19 ×œ-GDP.  
- CPI ×•××‘×˜×œ×” ×›××¢×˜ ×©××™× × ××¦×™×’×™× ×§×©×¨×™× ××•×‘×”×§×™× ×œ×©××¨ ×”××©×ª× ×™×.  

×œ×›×Ÿ, ×”×”×©×¤×¢×” ×”×›×œ×›×œ×™×ª ×©×œ ×”×§×•×¨×•× ×” **××™× ×” ××•×¤×™×¢×” ×‘×¦×•×¨×” ××•×‘×”×§×ª** ×‘×××’×¨ ×”× ×ª×•× ×™× ×”×–×”, ×•×–×” ××¡×‘×™×¨ ××“×•×¢ ××•×“×œ×™ ×”×¨×’×¨×¡×™×” ××¦×™×’×™× RÂ² × ××•×š â€” ×ª×•×¦××” ×ª×§×™× ×” ×œ×—×œ×•×˜×™×Ÿ ×‘×”×ª×—×©×‘ ×‘× ×ª×•× ×™×.







# LINEAR REGRESSION MULTIE MODEL TRAINING

```
# ============================================================
# ğŸ“Œ Regression â€“ Multi Model Training (Correct Version)
# ============================================================

# Features & target
X = df[['Confirmed', 'Deaths', 'Recovered', 'Unemployment', 'CPI']]
y = df['GDP']

# Train/Test Split (NO data leakage)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Scaling â€“ fit ONLY on train
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled  = scaler.transform(X_test)

# ============================================================
# 1ï¸âƒ£ Linear Regression
# ============================================================
lin_model = LinearRegression()
lin_model.fit(X_train_scaled, y_train)
lin_r2 = lin_model.score(X_test_scaled, y_test)

# ============================================================
# 2ï¸âƒ£ Ridge Regression (with CV)
# ============================================================
alphas = np.logspace(-3, 3, 50)

ridge = RidgeCV(alphas=alphas, cv=5, scoring='r2')
ridge.fit(X_train_scaled, y_train)
ridge_r2 = ridge.score(X_test_scaled, y_test)

# ============================================================
# 3ï¸âƒ£ Lasso Regression (with CV)
# ============================================================
lasso = LassoCV(alphas=alphas, cv=5, random_state=42)
lasso.fit(X_train_scaled, y_train)
lasso_r2 = lasso.score(X_test_scaled, y_test)

# ============================================================
# 4ï¸âƒ£ Polynomial Regression â€“ choose optimal degree using CV
# ============================================================
degrees = [1, 2, 3, 4]
poly_scores = []

for d in degrees:
    poly = make_pipeline(StandardScaler(), PolynomialFeatures(d), LinearRegression())
    cv_score = cross_val_score(poly, X, y, cv=5, scoring='r2').mean()
    poly_scores.append(cv_score)
    print(f"Degree {d} â†’ Mean CV RÂ²: {cv_score:.3f}")

best_degree = degrees[np.argmax(poly_scores)]

# Train final polynomial model using best degree
best_poly = make_pipeline(StandardScaler(), PolynomialFeatures(best_degree), LinearRegression())
best_poly.fit(X_train, y_train)
poly_r2 = best_poly.score(X_test, y_test)

# ============================================================
# 5ï¸âƒ£ Compare Models
# ============================================================
results = {
    "Linear Regression": lin_r2,
    "Ridge Regression": ridge_r2,
    "Lasso Regression": lasso_r2,
    f"Polynomial (deg={best_degree})": poly_r2
}

print("\n=== RÂ² Scores ===")
for name, score in results.items():
    print(f"{name}: {score:.4f}")

best_model_name = max(results, key=results.get)
print(f"\nğŸ† Best Model: {best_model_name}")

# Select final model
if best_model_name == "Linear Regression":
    final_model = lin_model
elif best_model_name == "Ridge Regression":
    final_model = ridge
elif best_model_name == "Lasso Regression":
    final_model = lasso
else:
    final_model = best_poly

# ============================================================
# 6ï¸âƒ£ Save model & scaler
# ============================================================
joblib.dump(final_model, "final_regression_model.joblib")
joblib.dump(scaler, "regression_scaler.joblib")

print("\nğŸ’¾ Regression model saved!")

# Reload test
loaded_model = joblib.load("final_regression_model.joblib")
loaded_scaler = joblib.load("regression_scaler.joblib")

print("âœ… Model and scaler loaded successfully.")

```

## ğŸ“˜ Summary & Discussion

×œ××—×¨ ×‘×“×™×§×ª ×›×œ×œ ×”××•×“×œ×™× (Linear, Ridge, Lasso, Polynomial), ××ª×§×‘×œ ×›×™ ×¢×¨×›×™ ×”Ö¾RÂ² × ××•×›×™× ×™×—×¡×™×ª.
×”××©××¢×•×ª ×”×™× ×©××“×“×™ ×”×‘×¨×™××•×ª ×©×œ ×”×§×•×¨×•× ×” (Confirmed, Deaths, Recovered) ×•×›×Ÿ ××“×“×™ ×”×××§×¨×• ×”×‘×¡×™×¡×™×™×
(CPI, Unemployment) **××™× × ××¡×•×’×œ×™× ×œ×”×¡×‘×™×¨ ×‘×¦×•×¨×” ×˜×•×‘×” ××ª ×”×©×•× ×•×ª ×‘-GDP ×‘×™×Ÿ ×”××“×™× ×•×ª**.

×›×œ ××¨×‘×¢×ª ×”××•×“×œ×™× ××¦×™×’×™× ×‘×™×¦×•×¢×™× ×“×•××™× ×××•×“, ×›××©×¨:
- **Linear Regression** ××¦×™×’ ××ª ×¢×¨×š ×”Ö¾RÂ² ×”×’×‘×•×” ×‘×™×•×ª×¨ (×’× ×× ×‘×¤×¢×¨ ×§×˜×Ÿ ×××•×“).  
- ×”×•×¡×¤×ª ×¨×’×•×œ×¨×™×–×¦×™×” (Ridge/Lasso) ×œ× ×©×™×¤×¨×” ××ª ×”×‘×™×¦×•×¢×™× ×‘××•×¤×Ÿ ××©××¢×•×ª×™.
- Polynomial Regression ×‘×“×¨×’×•×ª ×’×‘×•×”×•×ª ×§×¨×¡ ×œ×—×œ×•×˜×™×Ÿ (overfitting) â€” ×•×¨×§ ×“×¨×’×” 1 ×¢×‘×“×”, ×œ××¢×©×” ×›××• ××•×“×œ ×œ×™× ×™××¨×™.

### ğŸ¯ ××¡×§× ×”:
×”× ×ª×•× ×™× ××¨××™× ×›×™ **××™×Ÿ ×§×©×¨ ××•×‘×”×§ ×‘×™×Ÿ ××“×“×™ COVID-19 ×œ×‘×™×Ÿ ×¨××ª ×”×ª×•×¦×¨ (GDP)**.
×œ×›×Ÿ ××•×“×œ×™× ×œ×™× ×™××¨×™×™× ××• ×¤×•×œ×™× ×•××™×™× ××™× × ××¡×•×’×œ×™× ×œ×—×–×•×ª ××ª ×”-GDP ×‘×¦×•×¨×” ×˜×•×‘×” ×‘×¢×–×¨×ª ××©×ª× ×™× ××œ×• ×‘×œ×‘×“.

×›×“×™ ×œ×©×¤×¨ ××ª ×”×‘×™×¦×•×¢×™× ×™×© ×¦×•×¨×š ×œ×”×•×¡×™×£ ××©×ª× ×™× ×—×™×¦×•× ×™×™× ×›×’×•×Ÿ:
- ×’×•×“×œ ××•×›×œ×•×¡×™×™×”  
- ×”×™×§×£ ××¡×—×¨ ×‘×™× ×œ××•××™  
- ×”×•×¦××” ×××©×œ×ª×™×ª  
- ×©×™×¢×•×¨×™ ×¦××™×—×” ×§×•×“××™×  
- ×—×•×‘ ×œ××•××™  
- ××“×“×™ ×¤×™×ª×•×— (HDI)  

××•×“×œ×™× ××‘×•×¡×¡×™ ××’××•×ª ×××§×¨×•Ö¾×›×œ×›×œ×™×•×ª ×™×ª××™××• ×”×¨×‘×” ×™×•×ª×¨ ×œ××©×™××”.



---

<br><br><br>


<div align="center">
 



 <h1> ğŸ“Œ Customer Churn Prediction â€“ Full Project Code</h1>
âœ”ï¸ ×›×•×œ×œ Data Preparation, EDA, Training, Evaluation, Deployment
</div>

```python
# =========================================================
# ğŸ“¦ Imports â€“ ×›×œ ×”×™×™×‘×•× ××¨×•×›×– ×‘×ª×—×™×œ×ª ×”×§×•×“
# =========================================================
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, recall_score, f1_score,
    confusion_matrix
)
import joblib

# =========================================================
# ğŸ“Œ Data Preparation
# =========================================================

# ×˜×¢×™× ×ª ×”× ×ª×•× ×™×
path = "/content/drive/MyDrive/Classroom/×¢×•×ª×§ ×©×œ customer_churn_dataset.csv"
df = pd.read_csv(path)

df.head()
df.columns

# ×¢××•×“×ª CustomerID ×”×™× ××–×”×” ×‘×œ×‘×“ â†’ ×œ× × ×•×ª× ×ª ××™×“×¢ ×¢×œ ×”×ª× ×”×’×•×ª â†’ ××¡×™×¨×™×
df = df.drop(columns=["CustomerID"])
df.columns

# ×‘×“×™×§×ª ×¢×¨×›×™× ×—×¡×¨×™×
print("Missing Values:")
print(df.isnull().sum())

# ×‘×“×™×§×ª ×›×¤×™×œ×•×™×•×ª
print("Duplicate Rows:", df.duplicated().sum())

# ×§×™×“×•×“ ××©×ª× ×™× ×§×˜×’×•×¨×™××œ×™×™×
df = pd.get_dummies(df, drop_first=True)
df.head()


# =========================================================
# ğŸ“Š Data Exploration
# =========================================================

# ×—×™×©×•×‘ ×§×•×¨×œ×¦×™×”
corr = df.corr()
corr

# Heatmap
plt.figure(figsize=(14, 10))
sns.heatmap(corr, annot=True, cmap="coolwarm", fmt=".2f")
plt.title("Correlation Heatmap")
plt.show()

# Pairplot
sns.pairplot(df, height=2)
plt.show()


# =========================================================
# ğŸ“Œ Model Training â€“ Scaling + Splitting
# =========================================================

# X & y
X = df.drop("Churn", axis=1)
y = df["Churn"]

# ×—×œ×•×§×” ×œÖ¾Train/Test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# × ×¨××•×œ (Standardization)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

X_train_scaled[:5]


# =========================================================
# ğŸ¤– Logistic Regression â€“ Grid Search
# =========================================================

log_params = {
    'C': [0.01, 0.1, 1, 10],
    'penalty': ['l2'],
    'solver': ['lbfgs']
}

log_model = LogisticRegression(max_iter=500)
log_grid = GridSearchCV(log_model, log_params, cv=5, scoring='f1')
log_grid.fit(X_train_scaled, y_train)

print("Optimal Logistic Regression Parameters:")
print(log_grid.best_params_)

log_best = log_grid.best_estimator_


# =========================================================
# ğŸ¤– KNN â€“ Grid Search
# =========================================================

knn_params = {
    'n_neighbors': list(range(1, 21)),
    'weights': ['uniform', 'distance']
}

knn = KNeighborsClassifier()
knn_grid = GridSearchCV(knn, knn_params, cv=5, scoring='f1')
knn_grid.fit(X_train_scaled, y_train)

print("Optimal KNN Parameters:")
print(knn_grid.best_params_)

knn_best = knn_grid.best_estimator_


# =========================================================
# ğŸ¤– SVM â€“ Grid Search
# =========================================================

svm_params = {
    'C': [0.1, 1, 10],
    'kernel': ['rbf', 'linear'],
    'gamma': ['scale', 'auto']
}

svm = SVC(probability=True)
svm_grid = GridSearchCV(svm, svm_params, cv=5, scoring='f1')
svm_grid.fit(X_train_scaled, y_train)

print("Optimal SVM Parameters:")
print(svm_grid.best_params_)

svm_best = svm_grid.best_estimator_


# =========================================================
# ğŸŒ² Random Forest â€“ Grid Search
# =========================================================

rf_params = {
    'n_estimators': [50, 100, 200],
    'max_depth': [3, 5, 10, None],
    'min_samples_split': [2, 5, 10],
    'bootstrap': [True, False]
}

rf = RandomForestClassifier()
rf_grid = GridSearchCV(rf, rf_params, cv=5, scoring='f1')
rf_grid.fit(X_train, y_train)

print("Optimal Random Forest Parameters:")
print(rf_grid.best_params_)

rf_best = rf_grid.best_estimator_


# =========================================================
# ğŸ“Œ Predictions (×œ×¤×™ ×“×¨×™×©×•×ª ×”×¤×¨×•×™×§×˜)
# =========================================================

y_pred_log = log_best.predict(X_test_scaled)
y_proba_log = log_best.predict_proba(X_test_scaled)[:, 1]

y_pred_knn = knn_best.predict(X_test_scaled)
y_proba_knn = knn_best.predict_proba(X_test_scaled)[:, 1]

y_pred_svm = svm_best.predict(X_test_scaled)
y_proba_svm = svm_best.predict_proba(X_test_scaled)[:, 1]

y_pred_rf = rf_best.predict(X_test)
y_proba_rf = rf_best.predict_proba(X_test)[:, 1]


# =========================================================
# ğŸ“Œ Model Evaluation â€“ Accuracy, Recall, F1
# =========================================================

results = {"Model": [], "Accuracy": [], "Recall": [], "F1 Score": []}

def evaluate_model(name, y_test, y_pred):
    acc = accuracy_score(y_test, y_pred)
    rec = recall_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    results["Model"].append(name)
    results["Accuracy"].append(acc)
    results["Recall"].append(rec)
    results["F1 Score"].append(f1)

    print(f"\n{name}:")
    print("Accuracy:", acc)
    print("Recall:", rec)
    print("F1 Score:", f1)


evaluate_model("Logistic Regression", y_test, y_pred_log)
evaluate_model("KNN", y_test, y_pred_knn)
evaluate_model("SVM", y_test, y_pred_svm)
evaluate_model("Random Forest", y_test, y_pred_rf)


# =========================================================
# ğŸ“Œ Confusion Matrices
# =========================================================

def plot_confusion(model_name, y_test, y_pred):
    cm = confusion_matrix(y_test, y_pred)
    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, cmap="Blues", fmt="d")
    plt.title(f"Confusion Matrix â€“ {model_name}")
    plt.xlabel("Predicted")
    plt.ylabel("Actual")
    plt.show()

plot_confusion("Logistic Regression", y_test, y_pred_log)
plot_confusion("KNN", y_test, y_pred_knn)
plot_confusion("SVM", y_test, y_pred_svm)
plot_confusion("Random Forest", y_test, y_pred_rf)


# =========================================================
# ğŸ“Œ Model Comparison Table
# =========================================================

results_df = pd.DataFrame(results)
results_df


# =========================================================
# ğŸ† Selecting the Best Model
# =========================================================

best_model_name = results_df.iloc[results_df["F1 Score"].idxmax()]["Model"]
print("Best Model:", best_model_name)


# =========================================================
# ğŸ¯ Training Best Model on Full Dataset
# =========================================================

if best_model_name == "Logistic Regression":
    final_model = log_best
    X_all_scaled = scaler.fit_transform(X)
    final_model.fit(X_all_scaled, y)

elif best_model_name == "KNN":
    final_model = knn_best
    X_all_scaled = scaler.fit_transform(X)
    final_model.fit(X_all_scaled, y)

elif best_model_name == "SVM":
    final_model = svm_best
    X_all_scaled = scaler.fit_transform(X)
    final_model.fit(X_all_scaled, y)

elif best_model_name == "Random Forest":
    final_model = rf_best
    final_model.fit(X, y)  # RF ×œ× ×—×™×™×‘ Scaling


# =========================================================
# ğŸ’¾ Export Model + Scaler
# =========================================================

joblib.dump(final_model, "final_model.joblib")
joblib.dump(scaler, "scaler.joblib")

print("Model and scaler saved successfully!")


# =========================================================
# ğŸ”„ Loading Saved Model Back
# =========================================================

loaded_model = joblib.load("final_model.joblib")
loaded_scaler = joblib.load("scaler.joblib")

print("Model and scaler loaded successfully!")
```


# ğŸ“ Project II â€“ COVID-19 Economic Impact & Customer Churn Prediction  
### *Supervised Learning: Regression + Classification*

<p align="center">
  <img src="https://img.icons8.com/color/96/laptop.png" width="120">
</p>

×¤×¨×•×™×§×˜ ×–×” ××©×œ×‘ ×‘×™×Ÿ **×—×™×–×•×™ ×›×œ×›×œ×™** ×œ×‘×™×Ÿ **×—×™×–×•×™ × ×˜×™×©×ª ×œ×§×•×—×•×ª**, ×ª×•×š ×©×™××•×© ×‘××•×“×œ×™× ××ª×§×“××™× ×©×œ ×œ××™×“×ª ××›×•× ×”.

---

# ğŸ“ ×ª×•×›×Ÿ ×”×¢× ×™×™× ×™×
## **1. ×—×œ×§ ××³ â€“ ×¨×’×¨×¡×™×”: ×—×™×–×•×™ GDP**
## **2. ×—×œ×§ ×‘×³ â€“ ×¡×™×•×•×’: ×—×™×–×•×™ × ×˜×™×©×ª ×œ×§×•×—×•×ª (Churn)**
## **3. ×ª×•×‘× ×•×ª ××¨×›×–×™×•×ª**
## **4. ××¡×§× ×” ×¡×•×¤×™×ª**

---

# ğŸ“Œ ×—×œ×§ ××³ â€” ×¨×’×¨×¡×™×”: ×—×™×–×•×™ GDP

××˜×¨×ª ×”××©×™××”:  
×œ× ×ª×— ××ª ×”×§×©×¨ ×‘×™×Ÿ × ×ª×•× ×™ COVID-19 ×•××“×“×™× ×›×œ×›×œ×™×™× ×œ×‘×™×Ÿ **×”×ª×•×¦×¨ ×”××§×•××™ ×”×’×•×œ××™ (GDP)** ×©×œ ××“×™× ×•×ª.

---

# ğŸ“Š ×ª×•×‘× ×•×ª ××¨×›×–×™×•×ª â€” Correlation Analysis  

## **1ï¸âƒ£ ×§×©×¨ ×‘×™×Ÿ ××“×“×™ ×”×§×•×¨×•× ×” ×œÖ¾GDP**

×‘×©× ×™× **2021â€“2022**, × ××¦× ×›×™ ×”Ö¾GDP ××¦×™×’ **××ª×× ×—×œ×© ×××•×“** ×¢× ××“×“×™ ×”×§×•×¨×•× ×”:

- **Confirmed â†” GDP ~ 0.24**  
- **Deaths â†” GDP ~ 0.22**  
- **Recovered â†” GDP ~ 0.13**

### ğŸ§© ××©××¢×•×ª:
×”×§×•×¨×•× ×” **×œ× ××¡×‘×™×¨×” ××ª ×ª× ×•×“×•×ª ×”Ö¾GDP ×‘×™×Ÿ ××“×™× ×•×ª**.  
×”Ö¾GDP ××•×©×¤×¢ ××’×•×¨××™× ××”×•×ª×™×™× ×›××•:
- ×’×•×“×œ ×”×›×œ×›×œ×”  
- ××©××‘×™ ×˜×‘×¢  
- ×™×¦×•× ×•×™×‘×•×  
- ××“×™× ×™×•×ª ×××©×œ×ª×™×ª  
- ×¤×¢×™×œ×•×ª ×”×ª×¢×©×™×™×”  

×œ×›×Ÿ ×”××ª×× × ×©××¨ × ××•×š ×××•×“.

---

## **2ï¸âƒ£ ×§×©×¨×™× ×‘×™×Ÿ ××“×“×™ ×”×§×•×¨×•× ×”**

× ××¦× **×§×©×¨ ×—×–×§ ×××•×“** ×‘×™×Ÿ ××“×“×™ ×”××’×¤×”:

- **Confirmed â†” Recovered ~ 0.95**  
- **Confirmed â†” Deaths ~ 0.88â€“0.89**  
- **Recovered â†” Deaths ~ 0.79**

×”×™×’×™×•×Ÿ:  
×‘××“×™× ×•×ª ×¢× ×”×¨×‘×” × ×“×‘×§×™× â†’ ×™×”×™×• ×’× ×”×¨×‘×” ××—×œ×™××™× ×•×”×¨×‘×” × ×¤×˜×¨×™×.

---

## **3ï¸âƒ£ CPI ×•××‘×˜×œ×” â€” ××ª×× ×›××¢×˜ ××¤×¡×™**

×©× ×™ ×”××“×“×™× ×”×›×œ×›×œ×™×™×:
- ×œ× ×§×©×•×¨×™× ×œ×§×•×¨×•× ×”  
- ×œ× ×§×©×•×¨×™× ×œÖ¾GDP  
- × ×©×œ×˜×™× ×‘×¢×™×§×¨ ×××“×™× ×™×•×ª ×¤× ×™××™×ª, ×¡×’×¨×™× ×•××‘× ×” ×©×•×§ ×”×¢×‘×•×“×”  

---

# ğŸ“Œ ×—×œ×§ ×‘×³ â€” ×¡×™×•×•×’: ×—×™×–×•×™ × ×˜×™×©×ª ×œ×§×•×—×•×ª (Churn)

×‘××©×™××” ×–×• × ×‘× ×• ××•×“×œ×™× ×©××˜×¨×ª× ×œ×—×–×•×ª ×”×× ×œ×§×•×— ×™× ×˜×•×© ×©×™×¨×•×ª.

×”××•×“×œ×™× ×©× ×‘×“×§×•:
- Logistic Regression  
- KNN  
- SVM  
- Random Forest  

×‘×•×¦×¢×•:  
âœ”ï¸ Scaling  
âœ”ï¸ GridSearchCV  
âœ”ï¸ Confusion Matrix  
âœ”ï¸ Precision / Recall / F1  
âœ”ï¸ ×‘×—×™×¨×ª ××•×“×œ ×× ×¦×—  
âœ”ï¸ ×©××™×¨×ª ×”××•×“×œ ×œÖ¾Production  

---

# ğŸ† ×”××•×“×œ ×”×× ×¦×— â€” Random Forest

××•×“×œ ×–×” ×”×’×™×¢ ×œÖ¾**F1 ×”×’×‘×•×” ×‘×™×•×ª×¨**, ×¢× ×”×‘×™×¦×•×¢×™× ×”×™×¦×™×‘×™× ×‘×™×•×ª×¨.

---

# ğŸ§  ××¡×§× ×” ×¡×•×¤×™×ª

×”×¤×¨×•×™×§×˜ ××¦×™×’ ×ª×”×œ×™×š ××œ× ×©×œ Data Science:

- × ×™×§×•×™ ×•×˜×™×•×‘ ×“××˜×”  
- EDA ××¢××™×§  
- ××™××•×Ÿ ××•×“×œ×™× ××¨×•×‘×™×  
- ×”×©×•×•××ª ×‘×™×¦×•×¢×™×  
- ×‘×—×™×¨×ª ××•×“×œ ××™×˜×‘×™  
- ×©××™×¨×” ×•×˜×¢×™× ×” ×©×œ ××•×“×œ×™×  

---

# â­ ×¡×™×›×•× ×ª×•×¦××•×ª

## ğŸ”µ ×—×œ×§ ×”×¨×’×¨×¡×™×” (GDP)
- COVID ×œ× ××©×¤×™×¢ ×™×©×™×¨×•×ª ×¢×œ ×”Ö¾GDP  
- Ridge Regression ×¡×™×¤×§ ××ª ×”×™×¦×™×‘×•×ª ×”×’×‘×•×”×” ×‘×™×•×ª×¨  
- ×”××ª×××™× ×—×œ×©×™× â€“ ×ª×•×¦××” ×ª×§×™× ×” ×‘×”×ª×× ×œ× ×ª×•× ×™×  

## ğŸŸ¢ ×—×œ×§ ×”×¡×™×•×•×’ (Churn)
- ×”× ×ª×•× ×™× ×”×™×• ×—×–×§×™× ×•×‘×¨×•×¨×™×  
- Random Forest × ×ª×Ÿ ××ª ×ª×•×¦××•×ª ×”Ö¾F1 ×”×˜×•×‘×•×ª ×‘×™×•×ª×¨  
- ×”××•×“×œ ××ª××™× ×œ×©×™××•×© ×¢×¡×§×™ ×××™×ª×™  

---

# âœ”ï¸ ×¡×•×£
